---
title: "StatComp"
author: "Chunyan Xia"
date: "2020-12-16"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{StatComp}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


## Question
Exercises 3.3, 3.9, 3.10, and 3.13 (pages 94-95, Statistical Computating with R).

#### 3.3 

The $Pareto(2,2)$  distribution has cdf$$F(x)=1-(\frac b a)^a,\qquad x \geq b \geq0, a\geq 0$$Derive the probability inverse transformation $F^{-1}(U)$ and use the inverse transform method to simulate a random sample from the $Pareto(2,2)$ distribution. Graph the density histogram of the sample with the $Pareto(2,2)$ 
density superimposed for comparison.


#### 3.9 

The rescaled Epanechnikov kernel [85] is a symmetric density function$$f_e(x)=\frac 3 4 (1-x^2), \qquad |x| \leq 1 \tag {3.10}$$
Devroye and Gyorfi [71,p.236] give the following algorithm for simulation from this distribution. Generate $iid U_1,U_2,U_3 \sim Uniform(-1,1)$. If $|U_3|\geq|U_2|$ and $|U_3|\geq|U_1|$, deliver $U_2$;otherwise deliver $U_3$.Write a function to generate random variates from $f_e$, and construct the histogram density estimate of a large simulated random sample.

#### 3.10

Prove that the algorithm given in Exercise 3.9 generates variates from the density $f_e(3.10)$.

#### 3.13
It can be shown that the mixture in Exercise 3.12 has a Pareto distribution with cdf $$F(y)=1-(\frac {\beta} {\beta + y})^r, \qquad y \geq 0 $$(This is an alternative parameterization of the Pareto cdf given in Exercise 3.3.) Generate 1000 random observations from the mixture with $r=4 \qquad and \qquad \beta=2$. Compare the empirical and theoretical (Pareto) distributions by graphing the density histogram of the sample and superimposing the Pareto density curve.


## Answer

**3.3**

for $$F(x)=1-(\frac b a)^a, \qquad x \geq b \geq0, a\geq 0$$ then $$F^{-1}(U)=\frac b {(1-x)^{\frac 1 a}}, \qquad 0<x\leq 1$$ for $$a=2, b=2$$ then$$F^{-1}(U)=\frac 2 {(1-x)^{\frac 1 2}}, \qquad 0<x\leq 1$$

the density histogram of the sample with the $Pareto(2,2)$ 
density  as follow:
```{r,fig.width=8,fig.height=4}
n <- 1000
u <- runif(n)
x <- 2/((1-u)^(1/2)) # F(x) = 1-4/(x^2), x>=2
hist(x, prob = TRUE, main = expression(f(x)==8*x^{-3}),xlim = range(0,50)  )
y <- seq(2, 50, .01)
lines(y, 8*y^(-3)) 
```


**3.9 **


According to the algorithm, suppose that $iid\qquad X,Y,Z \sim U(-1,1)$

then $$f_e=\begin{cases} 
Y &if \qquad |Z|\geq |Y|\qquad and\qquad |Z| \geq|X|\\ 
Z & others
\end{cases}$$
the graph of histogram density estimate of a large simulated random sample (sample size = 1000)

```{r,fig.width=8,fig.height=4}
n <- 1000
fe <- c()
x <- runif(n, min = -1, max = 1)
y <- runif(n, min = -1, max = 1)
z <- runif(n, min = -1, max = 1)
for (i in 1 :1000){
  if(abs(z[i])>=abs(y[i]) && abs(z[i])>=abs(x[i])){fe[i] <- y[i]}else{fe[i] <- z[i]}}
hist(fe, prob = TRUE,main = (expression(f(x)==3/4*(1-x^2))))
u <- seq(-1,1, .01)
lines(u,3/4*(1-u^2))
```


#### 3.10

Prove that the algorithm given in Exercise 3.9 generates variates from the density $f_e(3.10)$.

$A_1=|U_3|\geq|U_2|$ $A_1=|U_3|\geq|U_1|$

when $-1\leq u\leq0$,$$F_U(u)=P(U_2\leq u|A_1A_2)+P(U_3\leq u)-P(U_3\leq u|A_1^CA_2^C)$$
$$\qquad \qquad = \frac 1 8\int^u_{-1}du_2\int^{u_2}_{-1}du_3\int^1_{u_3}du_1+(\frac {u+1} 2)-\frac 1 8 \int^u_{-1}du_3\int^{u_3}_{-1}du_1\int^{u_3}_{-1}du2$$
$$\qquad \qquad = -\frac 1 4x^3+ \frac 3 4 x+ \frac 1 2$$
when $0\leq u\leq1$,$$F_U(u)=P(U_2\leq u|A_1A_2)+P(U_3\leq u)-P(U_3\leq u|A_1^CA_2^C)$$
$$\qquad \qquad = -\frac 1 4x^3+ \frac 3 4 x+ \frac 1 2$$

#### 3.13
```{r,fig.width=8,fig.height=4}
n <- 1000  
u <- runif(n)
x <- 2/((1-u)^(1/4))-2 # F(y) = 1-(2/(2+y))^r, y>=0
hist(x, prob = TRUE, main = expression(f(y)==64/((2+y)^5)))
y <- seq(0, 100, .01)
lines(y, 64/((2+y)^5)) 
```



## Question
Exercises 5.1, 5.7, and 5.11 (pages 149-151, Statistical Computating with R).

#### 5.1 
 Compute a Monte Carlo estimate of $$\int^{\frac {\pi} 3 }_0 sint dt$$and compare your estimate with the exact value of the integral.

#### 5.7 

Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate θ by the antithetic variate approach and by the simple Monte Carlo method. Compute an empirical estimate of the percent reduction in variance using the antithetic variate. Compare the result with the theoretical value from Exercise 5.6. 


#### 5.11
 If $\hat{\theta}_1$ and $\hat{\theta}_2$ are unbiased estimators of $\theta$, and $\hat{\theta}_1$ and $\hat{\theta}_2$ are antithetic, we derived that $c^* = 1/2$ is the optimal constant that minimizes the variance of $\hat{\theta}_c=c\hat{\theta}_2+(1-c)\hat{\theta}_2$. Derive $c^*$ for the general case. That is, if $\hat{\theta}_1$ and $\hat{\theta}_2$ are any two unbiased estimators of θ, find the value $c^*$ that minimizes the variance of the estimator $\hat{\theta}_c=c\hat{\theta}_2+(1-c)\hat{\theta}_2$ in equation (5.11). ($c^∗$ will be
a function of the variances and the covariance of the estimators.)

## Answer

**5.1**
```{r}
m <- 1e5 ; x <- runif(m, min=0,max=pi/3)  #generate the random number
esti <- mean(pi/3*sin(x))   #calculate the estimate 
print(c(esti,1-cos(pi/3)))  # give the comparision of the estimate and the true value
```

**5.7**

```{r}
MC.Anti <- function( Q = 10000, anti = TRUE) {
u <- runif(Q/2)
if (!anti) v <- runif(Q/2) else
v <- 1 - u
u <- c(u, v)
g <- mean(exp(u))
g
}  #define a function for generating random number
m <- 1000
SMC <- AN <- numeric(m)
for (i in 1:m) {
SMC[i] <- MC.Anti( Q = 1000, anti = FALSE)
AN[i] <- MC.Anti( Q = 1000)
}   #SMC from simple Monte Carlo method and AN from antithetic variable approach 

print(sd(SMC))
print(sd(AN))
print((var(SMC) - var(AN))/var(SMC))

```



**5.11**

$\hat{\theta}_c=c\hat{\theta}_1+(1-c)\hat{\theta}_2=\hat{\theta}_2+c(\hat{\theta}_1-\hat{\theta}_2)$

$Var(\hat{\theta}_c)=Var(\hat{\theta}_2)+c^2Var(\hat{\theta}_1-\hat{\theta}_2)+2cCov(\hat{\theta}_2,\hat{\theta}_1-\hat{\theta}_2)$

$=(c\sqrt {Var(\hat{\theta}_1-\hat{\theta}_2)}+ \frac {Cov(\hat{\theta}_2,\hat{\theta}_1-\hat{\theta}_2)} {\sqrt {Var(\hat{\theta}_1-\hat{\theta}_2)}})^2+Var(\hat{\theta}_2)-\frac {Cov^2(\hat{\theta}_2,\hat{\theta}_1-\hat{\theta}_2)} {Var(\hat{\theta}_1-\hat{\theta}_2)}$

so,$c^*=-\frac {Cov(\hat{\theta}_2,\hat{\theta}_1-\hat{\theta}_2)} {\sqrt {Var(\hat{\theta}_1-\hat{\theta}_2)}}$



## Question
Exercises 5.13, 5.15, 6.4, and 6.5 (page 151 and 180,
Statistical Computating with R).

### 5.13 
 Find two importance functions f1 and f2 that are supported on $(1,\infty)$ and are ‘close’ to $$g(x)=\frac {x^2} {\sqrt{2 \pi}} e^{-x^2/2}, \qquad x>1.$$ Which of your two importance functions should produce the smaller variance in estimating $$\int^{\infty}_1 \frac {x^2} {\sqrt{2 \pi}} e^{-x^2/2} dx$$ by importance sampling? Explain.

### 5.15 
Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

### 6.4
Suppose that $X_{1}, \ldots, X_{n}$ are a random sample from a from a lognormal distribution with unknown parameters. Construct a $95 \%$ confidence interval for the parameter $\mu .$ Use a Monte Carlo method to obtain an empirical estimate of the confidence level.


### 6.5
Suppose a $95 \%$ symmetric $t$ -interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to $0.95 .$ Use a Monte Carlo experiment to estimate the coverage probability of the $t$ -interval for random samples of $\chi^{2}(2)$ data with sample size $n=20 .$ Compare your $t$ -interval results with the simulation results in Example $6.4 .$ (The $t$ -interval should be more robust to departures from normality than the interval for variance.)

## Answer

### 5.13 
```{r}
size <-10000 
the_est <- Se_the_est <- numeric(2) 
gx_function <- function(x) { (x^2)*exp(-x^2/2)/sqrt(2*pi)*(x>1) }

x1 <-rexp(size,1)   #f1=exp(-x)
f1gx <- gx_function(x1) / exp(-x1)
the_est[1] <- mean(f1gx)
Se_the_est[1] <- sd(f1gx)

x2 <- rlnorm(size,1) #f2=exp(-(log(x))^2/2)/(x*sqrt(2*pi))
f2 <- function(x) { exp(-(log(x))^2/2)/(x*sqrt(2*pi))}
f2gx <- gx_function(x2) / f2(x2)
the_est[2] <- mean(f2gx)
Se_the_est[2] <- sd(f2gx)

print(c(the_est, Se_the_est))
```
It can be seen from the results that the variance of the importance function $f_2$ is smaller, because the density function of the lognormal distribution is closer to $g(x)$ compared with the exponential distribution.

### 5.15 

```{r}
M <- 10000; 
k <- 5  # number of intervals
N <- 50    # size of stratified sampling
T2 <- numeric(k)
est <- matrix(0, N, 2)
g<-function(x) {exp(-x-log(1+x^2))*(x>0)*(x<1)}
for (i in 1:N) {
  u <- runif(M)
  u1 <- -log(1-u*(1-exp(-1)))
  fg <- g(u1)/(exp(-u1)/(1-exp(-1)))
  est[i, 1] <- mean(fg)
  for(j in 1:k) {
    u <- runif(M)
    x <- -log((exp(-(j-1)/5)-u*(exp(-(j-1)/5)-exp(-j/5))))
    f <-function(x) exp(-x)/((exp(-(j-1)/5)-exp(-j/5)))
    T2[j]<-mean(g(x)/(f(x)))
  }
    T3 <- sum(T2)
    est[i, 2] <- mean(T3)
}
apply(est,2,mean)
apply(est,2,sd)

```
It can be seen from the results that the variance of stratified sampling is smaller than that of importance sampling, while the variance of stratified importance sampling is the smallest.


### 6.4
```{r}
set.seed(12345)
size <- 50  # size of estimate
alp <- 0.05   # significance level
fx <- function(x) {
  Lcl =mean(x)-sd(x)*qt(1-alp/2,size-1)/sqrt(size) 
  Ucl = mean(x)+sd(x)*qt(1-alp/2,size-1)/sqrt(size)  
  return(c(Lcl,Ucl))
}     # give the expression of confidence interval
g <- replicate(1000, expr = {x <- log(rlnorm(size))
fx(x)})   # calculate confidence intervals
mean((0<g[2,]) & (0>g[1,]))  # check confidence intervals
```
The results are reliable.



### 6.5
```{r}
size <- 50  # size of estimate
alp <- 0.05   # significance level
fx <- function(x) {
  Lcl =mean(x)-sd(x)*qt(1-alp/2,size-1)/sqrt(size) 
  Ucl = mean(x)+sd(x)*qt(1-alp/2,size-1)/sqrt(size)  
  return(c(Lcl,Ucl))
}     # give the expression of confidence interval
g <- replicate(1000, expr = {x <- rchisq(size,2)
fx(x)})   # calculate confidence intervals
mean((2<g[2,]) & (2>g[1,]))  # check confidence intervals
```
Comparing the results of Example 6.4 and this question, obviously the results of Example 6.4 are more reliable.


## Question
Exercises 5.13, 5.15, 6.4, and 6.5 (page 151 and 180,
Statistical Computating with R).

### 5.13 
 Find two importance functions f1 and f2 that are supported on $(1,\infty)$ and are ‘close’ to $$g(x)=\frac {x^2} {\sqrt{2 \pi}} e^{-x^2/2}, \qquad x>1.$$ Which of your two importance functions should produce the smaller variance in estimating $$\int^{\infty}_1 \frac {x^2} {\sqrt{2 \pi}} e^{-x^2/2} dx$$ by importance sampling? Explain.

### 5.15 
Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

### 6.4
Suppose that $X_{1}, \ldots, X_{n}$ are a random sample from a from a lognormal distribution with unknown parameters. Construct a $95 \%$ confidence interval for the parameter $\mu .$ Use a Monte Carlo method to obtain an empirical estimate of the confidence level.


### 6.5
Suppose a $95 \%$ symmetric $t$ -interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to $0.95 .$ Use a Monte Carlo experiment to estimate the coverage probability of the $t$ -interval for random samples of $\chi^{2}(2)$ data with sample size $n=20 .$ Compare your $t$ -interval results with the simulation results in Example $6.4 .$ (The $t$ -interval should be more robust to departures from normality than the interval for variance.)

## Answer

### 5.13 
```{r}
size <-10000 
the_est <- Se_the_est <- numeric(2) 
gx_function <- function(x) { (x^2)*exp(-x^2/2)/sqrt(2*pi)*(x>1) }

x1 <-rexp(size,1)   #f1=exp(-x)
f1gx <- gx_function(x1) / exp(-x1)
the_est[1] <- mean(f1gx)
Se_the_est[1] <- sd(f1gx)

x2 <- rlnorm(size,1) #f2=exp(-(log(x))^2/2)/(x*sqrt(2*pi))
f2 <- function(x) { exp(-(log(x))^2/2)/(x*sqrt(2*pi))}
f2gx <- gx_function(x2) / f2(x2)
the_est[2] <- mean(f2gx)
Se_the_est[2] <- sd(f2gx)

print(c(the_est, Se_the_est))
```
It can be seen from the results that the variance of the importance function $f_2$ is smaller, because the density function of the lognormal distribution is closer to $g(x)$ compared with the exponential distribution.

### 5.15 

```{r}
M <- 10000; 
k <- 5  # number of intervals
N <- 50    # size of stratified sampling
T2 <- numeric(k)
est <- matrix(0, N, 2)
g<-function(x) {exp(-x-log(1+x^2))*(x>0)*(x<1)}
for (i in 1:N) {
  u <- runif(M)
  u1 <- -log(1-u*(1-exp(-1)))
  fg <- g(u1)/(exp(-u1)/(1-exp(-1)))
  est[i, 1] <- mean(fg)
  for(j in 1:k) {
    u <- runif(M)
    x <- -log((exp(-(j-1)/5)-u*(exp(-(j-1)/5)-exp(-j/5))))
    f <-function(x) exp(-x)/((exp(-(j-1)/5)-exp(-j/5)))
    T2[j]<-mean(g(x)/(f(x)))
  }
    T3 <- sum(T2)
    est[i, 2] <- mean(T3)
}
apply(est,2,mean)
apply(est,2,sd)

```
It can be seen from the results that the variance of stratified sampling is smaller than that of importance sampling, while the variance of stratified importance sampling is the smallest.


### 6.4
```{r}
set.seed(12345)
size <- 50  # size of estimate
alp <- 0.05   # significance level
fx <- function(x) {
  Lcl =mean(x)-sd(x)*qt(1-alp/2,size-1)/sqrt(size) 
  Ucl = mean(x)+sd(x)*qt(1-alp/2,size-1)/sqrt(size)  
  return(c(Lcl,Ucl))
}     # give the expression of confidence interval
g <- replicate(1000, expr = {x <- log(rlnorm(size))
fx(x)})   # calculate confidence intervals
mean((0<g[2,]) & (0>g[1,]))  # check confidence intervals
```
The results are reliable.



### 6.5
```{r}
size <- 50  # size of estimate
alp <- 0.05   # significance level
fx <- function(x) {
  Lcl =mean(x)-sd(x)*qt(1-alp/2,size-1)/sqrt(size) 
  Ucl = mean(x)+sd(x)*qt(1-alp/2,size-1)/sqrt(size)  
  return(c(Lcl,Ucl))
}     # give the expression of confidence interval
g <- replicate(1000, expr = {x <- rchisq(size,2)
fx(x)})   # calculate confidence intervals
mean((2<g[2,]) & (2>g[1,]))  # check confidence intervals
```
Comparing the results of Example 6.4 and this question, obviously the results of Example 6.4 are more reliable.


## Question
Exercises 6.7, 6.8, and 6.C (pages 180-182, Statistical Computating with R).

Discussion

### 6.7  

Estimate the power of the skewness test of normality against symmetric Beta(*α, α*) distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as *t*(*ν*)?

### 6.8  

Refer to Example 6.16. Repeat the simulation, but also compute the *F* test of equal variance, at significance level $\widehat α$ $\doteq$ 0*.*055. Compare the power of the Count Five test and *F* test for small, medium, and large sample sizes. (Recall that the *F* test is not applicable for non-normal distributions.)

### 6.C

Repeat Examples 6.8 and 6.10 for Mardia's multivariate skewness test. Mardia [187] proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If $X$ and $Y$ are iid, the multivariate population skewness $\beta_{1, d}$ is defined by Mardia as
$$
\beta_{1, d}=E\left[(X-\mu)^{T} \Sigma^{-1}(Y-\mu)\right]^{3}
$$
Under normality, $\beta_{1, d}=0 .$ The multivariate skewness statistic is
$$
b_{1, d}=\frac{1}{n^{2}} \sum_{i, j=1}^{n}\left(\left(X_{i}-\bar{X}\right)^{T} \widehat{\Sigma}^{-1}\left(X_{j}-\bar{X}\right)\right)^{3}
$$



## Question

Exercises 7.1, 7.5, 7.8, and 7.11 (pages 212-213, Statistical
Computating with R).

### 7.1  

Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2.

### 7.5  

Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the
mean time between failures $1/ \lambda$ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

### 7.8

Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard
error of $\hat {\theta}$.

### 7.11

In Example 7.18, leave-one-out (n-fold) cross validation was used to select the
best fitting model. Use leave-two-out cross validation to compare the models.

## Answer


### 7.1
```{r}
library(boot); data("law", package = "bootstrap");
n <- 15
b.cor <- function(x,i) cor(x[i,1],x[i,2])
x <- matrix(c(law$LSAT, law$GPA),15,2)

theta.hat <- b.cor(x,1:n)
theta.jack <- numeric(n)

for(i in 1:n){
theta.jack[i] <- b.cor(x,(1:n)[-i])
}
bias.jack <- (n-1)*(mean(theta.jack)-theta.hat)
se.jack <- sqrt((n-1)*mean((theta.jack-theta.hat)^2))
round(c(original=theta.hat,bias.jack=bias.jack,
se.jack=se.jack),3)
```
The bias is -0.006, the standard error is 0.143.


### 7.5 

```{r}
library(boot); data(aircondit, package = "boot");set.seed(1234)
boot.mean <- function(x,i) mean(x[i])
n <-12; m <- 1e2 
x <- aircondit$hours; ci.norm<-ci.basic<-ci.perc<-ci.bca<-matrix(NA,1,2)
for (i in 1: n){
  de <- boot(data=x,statistic=boot.mean, R = 999)
  ci <- boot.ci(de,type=c("norm","basic","perc","bca"))
}
ci

```
It can be seen from the results that standard bootstrap CI, Basic bootstrap CI, percentile bootstrap CI, and BCa Bootstrap CI are different, because of their different assumptions and principles.

The standard bootstrap CI based on asymptotic normality. The basic bootstrap CI based on the large sample property. Percentile CI (percent) by assuming $\hat {\theta}^*|data$ and $\hat {\theta}$ have approximately the same distribution. 
The BCa Bootstrap CI is based on the improvement of percentile  bootstrap CI, which is mainly about bias corrected and adjusted for acceleration. Compared with percentile  bootstrap CI, the BCa Bootstrap CI has better theoretical properties and actual performance.



### 7.8

```{r}
library(boot);data("scor", package = "bootstrap")
n <- 88
# define correlation and eigen function to simplify calculation
b.eig <- function(x,i) eigen(cor(x))$values
eig.hat <- b.eig(scor)
theta.hat <- max(eig.hat)/sum(eig.hat)

# bootstrap method
eig.jack <- matrix(NA, n,5)
theta.jack <- numeric(n)
for(i in 1:n){
  score <- scor[(1:n)[-i], ]
  eig.jack[i, ] <- b.eig(score)
  theta.jack[i] <- max(eig.jack[i, ])/sum(eig.jack[i, ])
}

bias.jack <- (n-1)*(mean(theta.jack)-theta.hat)
se.jack <- sqrt((n-1)*mean((theta.jack-theta.hat)^2))
round(c(original=theta.hat,bias.jack=bias.jack,
se.jack=se.jack),5)

```
The bias of $\hat {\theta}$ is -0.00038, the standard error of $\hat {\theta}$ is 0.04467.


### 7.11

```{r message= FALSE}
library(DAAG);  attach(ironslag)  #data from DAAG ironslag
n <- length(magnetic)
e1 <- e2 <- e3 <- e4 <- matrix(0,n,n)

# fit models on leave-two-out samples of n-fold cross validation
for (k in 1:(n-1)) {
  for (i in (k+1):n) {
    y <- magnetic[-c(k,i)]
    x <- chemical[-c(k,i)]
    
  # the linear model  
  J1 <- lm(y ~ x)
  yhat1 <- J1$coef[1]+J1$coef[2]*chemical[c(k,i)]
  e1[k,i] <- mean((magnetic[c(k,i)]-yhat1)^2)
  
  # the quadratic model
  J2 <- lm(y~x+I(x^2))
  yhat2 <- J2$coef[1]+J2$coef[2]*chemical[c(k,i)]+
    J2$coef[3]*chemical[c(k,i)]^2
  e2[k,i] <- mean((magnetic[c(k,i)]-yhat2)^2)
  
  # the exponential model
  J3 <- lm(log(y)~x)
  logyhat3 <- J3$coef[1]+J3$coef[2]*chemical[c(k,i)]
  yhat3 <- exp(logyhat3)
  e3[k,i] <- mean((magnetic[c(k,i)]-yhat3)^2)
  
  # log-log model
  J4 <- lm(log(y)~log(x))
  logyhat4 <- J4$coef[1]+J4$coef[2]*log(chemical[c(k,i)])
  yhat4 <- exp(logyhat4)
  e4[k,i] <- mean((magnetic[c(k,i)]-yhat4)^2)
  }
}
  # compute the MSE of models
c(2*sum(e1)/(n*(n-1)),2*sum(e2)/(n*(n-1)),2*sum(e3)/(n*(n-1)),2*sum(e4)/(n*(n-1)))
```
From leave-two-out cross validation, we can know that the second model, quadratic model, has the minimal MSE, whose MSE is 17.87.
Under the method of leave-Two-out Cross Validation, it is the model with the optimal fitting.
The second is the exponential model, with an MSE of 18.45.
Then, the linear model, whose MSE is 19.57. 
The worst fitted model is the log-log model, whose MSE is 20.47.

Compared with the results in Example 7.18, it is found that the conclusion of leave-one-out cross validation and leave-two-out cross validation are the same.




## Question

### 8.3

The Count 5 test for equal variances in Section 6.4 is based on the maximum
number of extreme points. Example 6.15 shows that the Count 5 criterion
is not applicable for unequal sample sizes. Implement a permutation test for
equal variance based on the maximum number of extreme points that applies
when sample sizes are not necessarily equal.

### Design experiments

Design experiments for evaluating the performance of the NN,
energy, and ball methods in various situations.

Unequal variances and equal expectations

Unequal variances and unequal expectations

Non-normal distributions: t distribution with 1 df (heavy-tailed
distribution), bimodel distribution (mixture of two normal
distributions)

Unbalanced samples (say, 1 case versus 10 controls)

Note: The parameters should be chosen such that the powers
are distinguishable (say, range from 0.3 to 0.8).



## Answer


### 8.3

The Count 5 test for equal variances in Section 6.4 is based on the maximum
number of extreme points. Example 6.15 shows that the Count 5 criterion
is not applicable for unequal sample sizes. Implement a permutation test for
equal variance based on the maximum number of extreme points that applies
when sample sizes are not necessarily equal.

```{r}
 count5test <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
# return 1 (reject) or 0 (do not reject H0)
return(as.integer(max(c(outx, outy)) > 5))
 }
n1 <- 20;   n2 <- 30;  set.seed(1234)
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
m <- 10000
K <- 1:50

tests <- replicate(m, expr = {
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
z <- c(x,y)
kx <- sample(K, size = n1, replace = FALSE)
ky <- sample(K, size = n2, replace = FALSE)

x1 <- z[kx];y1 <- z[-ky]
x1 <- x1 - mean(x1) 
y1 <- y1 - mean(y1)
count5test(x1, y1)
} )
alphahat <- mean(tests)
alphahat 

```
The result is about 0.0224, which is smaller than 0.05. So the test is efficient.


### Design experiments
```{r}
set.seed(123)
library(RANN); library(energy);library(Ball)
Tn <- function(z, ix, sizes,k) {
n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
if(is.vector(z)) z <- data.frame(z,0);
z <- z[ix, ];
NN <- nn2(data=z, k=k+1) 
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
(i1 + i2) / (k * n)
}

eqdist.nn <- function(z,sizes,k){
boot.obj <- boot(data=z,statistic=Tn,R=R,
sim = "permutation", sizes = sizes,k=k)
ts <- c(boot.obj$t0,boot.obj$t)
p.value <- mean(ts>=ts[1])
list(statistic=ts[1],p.value=p.value)
}

#  1.For unequal variances and equal expectations
m <- 1e2; k<-3; p<-2; mu <- 0.3
n1 <- n2 <- 50; R<-999; n <- n1+n2; N = c(n1,n2)
p.values <- matrix(NA,m,3)

for(i in 1:m){
  x <- matrix(rnorm(n1*p,0,1),ncol=p);
  y <- cbind(rnorm(n2,0,1.2),rnorm(n2,0,1.2));
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=999)$p.value
  p.values[i,3] <-bd.test(x=x,y=y,num.permutations=999,seed=i*12345)$p.value
}
alpha <- 0.1;
pow1 <- colMeans(p.values<alpha)
pow1


#  2.For unequal variances and unequal expectations

m <- 1e2; k<-3; p<-2; mu <- 0.3
n1 <- n2 <- 50; R<-999; n <- n1+n2; N = c(n1,n2)
p.values <- matrix(NA,m,3)

for(i in 1:m){
  x <- matrix(rnorm(n1*p,0,1),ncol=p);
  y <- cbind(rnorm(n2,0.1,1.2),rnorm(n2,0.1,1.2));
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=999)$p.value
  p.values[i,3] <-bd.test(x=x,y=y,num.permutations=999,seed=i*12345)$p.value
}
alpha <- 0.1;
pow2 <- colMeans(p.values<alpha)
pow2


#  3. Non-normal distributions(t distribution with 1 df (heavy-tailed distribution) and bimodel distribution (mixture of two normal distributions)) 

m <- 1e2; k<-3; p<-2; mu <- 0.3
n1 <- n2 <- 10; R<-999; n <- n1+n2; N = c(n1,n2)
p.values <- matrix(NA,m,3)

for(i in 1:m){
 x <- matrix(rt(n1*p,1),ncol = p);
  y <- cbind(rnorm(n2),rnorm(n2,mean = 1))
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=999)$p.value
  p.values[i,3] <-bd.test(x=x,y=y,num.permutations=999,seed=i*12345)$p.value
}
alpha <- 0.1;
pow3 <- colMeans(p.values<alpha)
pow3



#  4. For unbalanced samples (say, 1 case versus 10 controls) 
m <- 100; k<-3; p<-2; mu <- 0.3; set.seed(345)
n1 <- 30; n2 <- 20; R<-999; n <- n1+n2; N = c(n1,n2)
eqdist.nn <- function(z,sizes,k){
boot.obj <- boot(data=z,statistic=Tn,R=R,
sim = "permutation", sizes = sizes,k=k)
ts <- c(boot.obj$t0,boot.obj$t)
p.value <- mean(ts>=ts[1])
list(statistic=ts[1],p.value=p.value)
}
p.values <- matrix(NA,m,3)
for(i in 1:m){
x <- matrix(rnorm(n1*p,0,3.5),ncol=p);
y <- matrix(rnorm(n2*p,0,2),ncol=p);
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,num.permutations=999,seed=i*1)$p.value
}
alpha <- 0.1;
pow4 <- colMeans(p.values<alpha)
pow4

```


For unequal variances and equal expectations, we can see that the ball method has the best power, which is 0.37. Then the power of energy method is 0.18. The power of NN method has the smallest power, which is 0.16.

For unequal variances and unequal expectations,we can see that the ball method has the best power, which is 0.42. Then the power of NN method is 0.21. The power of energy method has the smallest power, which is 0.19.

For non-normal distributions(t distribution with 1 df (heavy-tailed distribution) and bimodel distribution (mixture of two normal distributions)), we can see that the ball method has the best power, which is 0.76. Then the power of energy method is 0.67. The power of NN method has the smallest power, which is 0.38.


For unbalanced samples, in this case, we can see that the ball method has the best power, which is 0.86. Then the power of NN method is 0.41. The power of energy method has the smallest power, which is 0.39.


## Question

Exercies 9.4 (pages 277, Statistical Computating with R).

For Exercise 9.4, use the Gelman-Rubin method to monitor
convergence of the chain, and run the chain until it converges
approximately to the target distribution according to $\hat{R}<1.2$.

Exercises 11.4 (pages 353, Statistical Computing with R)

### 9.4

Implement a random walk Metropolis sampler for generating the standard
Laplace distribution (see Exercise 3.2). For the increment, simulate from a
normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of
each chain.

### For 9.4

For Exercise 9.4, use the Gelman-Rubin method to monitor
convergence of the chain, and run the chain until it converges
approximately to the target distribution according to $\hat{R}<1.2$.

### 11.4

Find the intersection points $A(k)$ in $(0, \sqrt k)$ of the curves $$S_{(k-1)}(a)=P(t(k-1)> \sqrt {\frac {a^2(k-1)}  {k-a^2} })$$
and$$S_k(a)=P(t(k)> \sqrt {\frac {a^2k}  {k+1-a^2} })$$,
for $k = 4:25, 100, 500, 1000$, where t(k) is a Student t random variable with
k degrees of freedom. (These intersection points determine the critical values
for a t-test for scale-mixture errors proposed by $Sz\acute{e}kely$ [260].)

## Answer


### 9.4

Implement a random walk Metropolis sampler for generating the standard
Laplace distribution (see Exercise 3.2). For the increment, simulate from a
normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of
each chain.

```{r}
rw.Metropolis <- function(sigma, x0, N) {
x <- numeric(N)
x[1] <- x0
u <- runif(N)
k <- 0
for (i in 2:N) {
y <- rnorm(1, x[i-1], sigma)
if (u[i] <= exp(abs(x[i-1])-abs(y)))
x[i] <- y else {
x[i] <- x[i-1]
k <- k + 1
} }
return(list(x=x, k=k))
}



N <- 2000
sigma <- c(.05, .25, 1, 5)
x0 <- 20
rw1 <- rw.Metropolis(sigma[1], x0, N)
rw2 <- rw.Metropolis(sigma[2], x0, N)
rw3 <- rw.Metropolis(sigma[3], x0, N)
rw4 <- rw.Metropolis(sigma[4], x0, N)

print(c(rw1$k, rw2$k, rw3$k, rw4$k))
```
According to the results, the acceptance rate of chain was between [0.15, 0.5] only when $\sigma = 1$.
Other values of $\sigma$acceptance rates are not within this range.


### For 9.4

For Exercise 9.4, use the Gelman-Rubin method to monitor
convergence of the chain, and run the chain until it converges
approximately to the target distribution according to $\hat{R}<1.2$.

```{r}
# function of Gelman.Rubin
Gelman.Rubin <- function(psi) {
# psi[i,j] is the statistic psi(X[i,1:j])
# for chain in i-th row of X
psi <- as.matrix(psi)
n <- ncol(psi)
k <- nrow(psi)
psi.means <- rowMeans(psi) #row means
B <- n * var(psi.means) #between variance est.
psi.w <- apply(psi, 1, "var") #within variances
W <- mean(psi.w) #within est.
v.hat <- W*(n-1)/n + (B/n) #upper variance est.
r.hat <- v.hat / W #G-R statistic
return(r.hat)
}


laplace.chain <- function(sigma, N, X1) {
#generates a Metropolis chain for Normal(0,1)
#with Normal(X[t], sigma) proposal distribution
#and starting value X1
x <- numeric(N)
x[1] <- X1
u <- runif(N)
for (i in 2:N) {
  y <- rnorm(1, x[i-1], sigma)
if (u[i] <= exp(abs(x[i-1])-abs(y))) x[i] <- y else
x[i] <- x[i-1]
}
return(x)
}



GR_method_analysis <- function(sigma) {
k <- 4 #number of chains to generate
n <- 15000 #length of chains
b <- 1000 #burn-in length

#choose overdispersed initial values
x0 <- c(-10, -5, 5, 10)
#generate the chains
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k)
X[i, ] <- laplace.chain(sigma, n, x0[i])

#compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
psi[i,] <- psi[i,] / (1:ncol(psi))
print(Gelman.Rubin(psi))
} 

x0 <- c(-10, -5, 5, 10)
sigma_.05 <- GR_method_analysis(0.05)
sigma_.25 <- GR_method_analysis(0.25)
sigma_1 <- GR_method_analysis(1)
sigma_5 <- GR_method_analysis(5)

```

When $\sigma =0.025 or 0.25$, the ratios $r(X_t, Y)$ tend to be
large and almost every candidate point is accepted. The increments are small now. The chain in the second plot generated with
When $\sigma = 1$, it converges very slowly and requires a much longer burn-in period.The chain is mixing well and converging to the target
distribution after a short burn-in period of about 1000. $\sigma = 5$ is the same, and it converges to the target distrubution after about 1500.


### 11.4

Find the intersection points $A(k)$ in $(0, \sqrt k)$ of the curves $$S_{(k-1)}(a)=P(t(k-1)> \sqrt {\frac {a^2(k-1)}  {k-a^2} })$$
and$$S_k(a)=P(t(k)> \sqrt {\frac {a^2k}  {k+1-a^2} })$$,
for $k = 4:25, 100, 500, 1000$, where t(k) is a Student t random variable with
k degrees of freedom. (These intersection points determine the critical values
for a t-test for scale-mixture errors proposed by $Sz\acute{e}kely$ [260].)


```{r}
ck <- function(k,a){
  sqrt(a^2*k/(k+1-a^2))
}
equ <- function(k,a){
  pt(ck(k-1,a),df = k-1) - pt(ck(k,a),df = k)
}
root.curve <- sapply(c(4:25,100,500,1000),function(k){uniroot(equ,interval = c(0.1,sqrt(k)-0.1),k=k)$root})

root.curve
```




## Question

A-B-O blood type problem

Exercises 3 (page 204, Advanced R)

Excecises 3 and 6 (page 213-214, Advanced R)

### A-B-O blood type problem
Let the three alleles be A, B, and O.


| Genotype  | AA   | BB   | OO   | AO   | BO   | AB   | Sum  |
| --------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| Frequency | p^2  | q^2  | r^2  | 2pr  | 2qr  | 2pq  | 1    |
| Count     | nAA  | nBB  | nOO  | nAO  | nBO  | nAB  | n    |

Observed data: $n_{A·} = n_{AA} + n_{AO} = 444 (A-type)$,
$n_{A·} = n_{AA} + n_{AO} = 132 (B-type)$, $n_{OO} = 361 (O-type)$,
$n_{AB} = 63 (AB-type)$.

Use EM algorithm to solve MLE of p and q (consider missing
data $n_{AA}$ and $n_{BB})$.

Record the values of p and q that maximize the conditional
likelihood in each EM steps, calculate the corresponding
log-maximum likelihood values (for observed data), are they
increasing?


### Exercises 3 (page 204, Advanced R)

Use both for loops and lapply() to fit linear models to the
mtcars using the formulas stored in this list:

formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)

### Excecises 3 (page 213, Advanced R)

The following code simulates the performance of a t-test for
non-normal data. Use sapply() and an anonymous function
to extract the p-value from every trial.
trials <- replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
Extra challenge: get rid of the anonymous function by using
[[ directly.

### Excecises 6 (page 214, Advanced R)

Implement a combination of Map() and vapply() to create an
lapply() variant that iterates in parallel over all of its inputs
and stores its outputs in a vector (or a matrix). What arguments should the function take?

## Answer


### A-B-O blood type problem




```{r}
set.seed(1234)
n_a. <- 444
n_b. <- 132
n_ab <- 63
n_oo <- 361
p0 <- runif(1,0,1)
q0 <- runif(1,0,1-p0)

likelihood_e <- function(prob,p0,q0){
  r0 <- 1-p0-q0 
  p <- prob[1]; q <- prob[2] ; r <- 1-p-q
  - n_a. * (2*log(p)*(p0^2/(p0^2+2*p0*r0)) + log(2*p*r)*(2*p0*r0/(p0^2+2*p0*r0))) -
    n_b. * (2*log(q)*(q0^2/(q0^2+2*q0*r0)) + log(2*q*r)*(2*q0*r0/(q0^2+2*q0*r0))) -
    n_ab * log(2*p*q) - 2*n_oo * log(r) 
}

iter <- 0;E1 <- 0;E2 <- 1; N <- 10
likelihood_trace <- p_maxtrace <- q_maxtrace <- numeric(N)
while(iter < N && abs(E1-E2)> 1e-5){
  output <- optim(par = c(0.1,0.1),likelihood_e,p0 = p0,q0 = q0)
  E1 <- E2;E2 <- output$value
  p0 <- output$par[1]
  q0 <- output$par[2]
  iter <- iter + 1
  p_maxtrace[iter+1] <- output$par[1]
  q_maxtrace[iter+1] <- output$par[2]
  likelihood_trace[iter+1] <- output$value
}
estimate <- data.frame(p0,q0,iter)
colnames(estimate) <- c("p","q","iteration times")
knitr::kable(estimate)
maxtrace <- data.frame(p_maxtrace,q_maxtrace,-likelihood_trace)
colnames(estimate) <- c("p_maxtrace","q_maxtrace","ikelihood_trace")
knitr::kable(maxtrace)

```
The results increase.



### Exercises 3 (page 204, Advanced R)

```{r}
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp +wt,
  mpg ~ I(1 / disp) + wt
)

# lapply (2 versions)
la1 <- lapply(formulas, lm, data = mtcars) 
la2 <- lapply(formulas, function(x) lm(formula = x, data = mtcars)) 
# for loop
lf1 <- vector("list", length(formulas)) 
for (i in seq_along(formulas)){ 
 lf1[[i]] <- lm(formulas[[i]], data = mtcars) 
}

```


### Excecises 3 (page 213, Advanced R)

```{r}
trials <- replicate(
  100,
  t.test(rpois(10,10),rpois(7,10)),
  simplify = FALSE
)

round(sapply(1:100,function(i){trials[[i]]$p.value}),3)

# without anonymous function
round(sapply(trials,"[[","p.value"),3)

```


### Excecises 6 (page 214, Advanced R)

```{r}
example_list <- list(iris, mtcars, cars) 
lapply(example_list, function(x) vapply(x, mean, numeric(1)))
map_vapply <- function(X, FUN, FUN.VALUE, simplify = FALSE){ 
 out <- Map(function(x) vapply(x, FUN, FUN.VALUE), X) 
 if(simplify == TRUE){return(simplify2array(out))} 
 out 
} 
map_vapply(example_list, mean, numeric(1))
```
As we can see, the result is the same.



You have already written an R function for Exercise 9.4 (page 277, Statistical Computing with R). Rewrite an Rcpp function for the same task. 

1. Compare the generated random numbers by the two functions using qqplot. 

2. Campare the computation time of the two functions with microbenchmark. 

3. Comments your results.


### Rcpp

```{r,eval=FALSE}

#include <cmath>
#include <Rcpp.h>
using namespace Rcpp;

//[[Rcpp::export]]
double f(double x) {
  return exp(-abs(x));
}

//[[Rcpp::export]]
NumericVector rwMetropolis (double sigma, double x0, int N) {
  NumericVector x(N);
  x[0] = x0; 
  NumericVector u = runif(N);
  for (int i = 1; i < N;i++ ) {
    NumericVector y = rnorm(1, x[i-1], sigma);
    if (u[i] <= (f(y[0]) / f(x[i-1]))){
      x[i] = y[0];
    }
    else { 
      x[i] = x[i-1]; 
    }
  }
  return(x);
} 

```

### comparison of the computation time

```{r, eval=FALSE}
    library(Rcpp)
    library(microbenchmark)
    # R
    lap_f = function(x) exp(-abs(x))

    rw.Metropolis = function(sigma, x0, N){
    x = numeric(N)
    x[1] = x0
    u = runif(N)
    k = 0
    for (i in 2:N) {
    y = rnorm(1, x[i-1], sigma)
    if (u[i] <= (lap_f(y) / lap_f(x[i-1]))) x[i] = y 
    else {
    x[i] = x[i-1]
    k = k+1
     }
    }
     return(list(x = x, k = k))
    }

    dir_cpp = 'F:/Rcpp/StatComp20028/src/'
    sourceCpp(paste0(dir_cpp,"rwMetropolis.cpp"))
    x0 = 25
    N = 2000
    sigma = 2
    (time = microbenchmark(rwR=rw.Metropolis(sigma,x0,N),rwC=rwMetropolis(sigma,x0,N)))
```

We can see that  the running time of using Cpp functionis much shorter than using R function.So using Rcpp method can improve computing efficiency.

### qqplot

```{r,eval=FALSE}

set.seed(12345)
rwR = rw.Metropolis(sigma,x0,N)$x[-(1:500)]
rwC = rwMetropolis(sigma,x0,N)[-(1:500)]
qqplot(rwR,rwC)
abline(a=0,b=1,col='black')
```

The dots of qqplot are located close to the diagonal lines. The random numbers generated by the two functions  are similar.